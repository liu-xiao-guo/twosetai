{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1750a71-a471-4a24-b490-559dc45bce6d",
   "metadata": {},
   "source": [
    "# Agentic RAG\n",
    "\n",
    "The main points of an Agent-based RAG solution are:\n",
    "\n",
    "- **Agentic**: The system is autonomous, making decisions and taking actions based on the context of the interaction.\n",
    "- **RAG (Retrieval-Augmented Generation):** Combines information retrieval from the knowledge base with the LLM’s generative capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b1e46a-2181-4f5c-9c01-2359346e8e54",
   "metadata": {},
   "source": [
    "## System architecture\n",
    "\n",
    "\n",
    "![agentic rag](images/agentic_rag_0.png)\n",
    "![agentic rag](images/agentic_rag_1.png)\n",
    "![agentic rag](images/agentic_rag_2.png)\n",
    "![agentic rag](images/agentic_rag_3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af0e2c-3d72-448d-b065-9e643abff513",
   "metadata": {},
   "source": [
    "## Langchain Code\n",
    "\n",
    "![](images/langchain_agentic_rag.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f7bc5e-a02c-4db9-80a1-273f481e4d0b",
   "metadata": {},
   "source": [
    "## Read and load PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2ccfda-74d5-4343-bbcc-9abff63b5cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'liuxgn.local', 'cluster_name': 'elasticsearch', 'cluster_uuid': 'ofa0pMjxSzyxatVvJBMEvQ', 'version': {'number': '9.3.0', 'build_flavor': 'default', 'build_type': 'tar', 'build_hash': '17b451d8979a29e31935fe1eb901310350b30e62', 'build_date': '2026-01-29T10:05:46.708397977Z', 'build_snapshot': False, 'lucene_version': '10.3.2', 'minimum_wire_compatibility_version': '8.19.0', 'minimum_index_compatibility_version': '8.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from elasticsearch import Elasticsearch\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ES_USER= os.getenv(\"ES_USER\")\n",
    "ES_PASSWORD = os.getenv(\"ES_PASSWORD\")\n",
    "ES_ENDPOINT = os.getenv(\"ES_ENDPOINT\")\n",
    "\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\")\n",
    "AZURE_EMBEDDING_ENDPOINT = os.getenv(\"AZURE_EMBEDDING_ENDPOINT\")\n",
    "AZURE_EMBEDDING_API_KEY = os.getenv(\"AZURE_EMBEDDING_API_KEY\")\n",
    "AZURE_EMBEDDING_API_VERSION = os.getenv(\"AZURE_EMBEDDING_API_VERSION\")\n",
    "\n",
    "AZURE_API_KEY = os.getenv(\"AZURE_API_KEY\")\n",
    "AZURE_EDNPOINT = os.getenv(\"AZURE_EDNPOINT\")\n",
    "AZURE_API_VERSION = os.getenv(\"AZURE_API_VERSION\")\n",
    "AZURE_DEPLOYMENT_ID = os.getenv(\"AZURE_DEPLOYMENT_ID\")\n",
    "\n",
    "TAVILIO_API_KEY = os.getenv(\"TAVILIO_API_KEY\")\n",
    "\n",
    "url = f\"https://{ES_USER}:{ES_PASSWORD}@{ES_ENDPOINT}:9200\"\n",
    "es = Elasticsearch(url, ca_certs = \"./http_ca.crt\", verify_certs = True)\n",
    "\n",
    "print(es.info())\n",
    "\n",
    "# all_documents = read_pdfs_from_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7704935f-6c9a-4976-ad8e-b3a2e905e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_index_name = \"agent_rag_index\"\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=MODEL_NAME,\n",
    "    azure_endpoint=AZURE_EMBEDDING_ENDPOINT, \n",
    "    api_key= AZURE_EMBEDDING_API_KEY,\n",
    "    openai_api_version=AZURE_EMBEDDING_API_VERSION\n",
    ")\n",
    "\n",
    "chat = AzureOpenAI(\n",
    "  api_key = AZURE_API_KEY,  \n",
    "  api_version = AZURE_API_VERSION,\n",
    "  azure_endpoint = AZURE_EDNPOINT\n",
    ")\n",
    "\n",
    "\n",
    "def read_pdfs_from_folder(folder_path):\n",
    "    pdf_list = []\n",
    "    \n",
    "    # Loop through all files in the specified folder\n",
    "    for filename in tqdm(os.listdir(folder_path)):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Open each PDF file\n",
    "            with open(file_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                content = \"\"\n",
    "                \n",
    "                # Read each page's content and append it to a string\n",
    "                for page_num in range(len(reader.pages)):\n",
    "                    page = reader.pages[page_num]\n",
    "                    content += page.extract_text()\n",
    "                \n",
    "                # Add the PDF content to the list\n",
    "                pdf_list.append({\"content\": content, \"filename\": filename})\n",
    "    \n",
    "    return pdf_list\n",
    "\n",
    "folder_path = \"./rag_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecd92b5a-8f3e-4bee-bea5-bd70864076f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureOpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x10eea9110>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x10ef39c10>, model='text-embedding-ada-002', dimensions=None, deployment=None, openai_api_version='2023-05-15', openai_api_base=None, openai_api_type='azure', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=2048, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True, azure_endpoint='https://ada-embeddings1.openai.azure.com/', azure_ad_token=None, azure_ad_token_provider=None, validate_base_url=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4cc9b3-1131-4392-8ee9-7c019b353cfb",
   "metadata": {},
   "source": [
    "## Read Web URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d74db54-5f22-47ee-bfd4-6a987be88ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import requests\n",
    "\n",
    "def fetch_url_content(url: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Fetches content from a URL by performing an HTTP GET request.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The endpoint or URL to fetch content from.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The content retrieved from the URL as a string,\n",
    "                       or None if the request fails.\n",
    "    \"\"\"\n",
    "    prefix_url: str = \"https://r.jina.ai/\"\n",
    "    full_url: str = prefix_url + url  # Concatenate the prefix URL with the provided URL\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(full_url)  # Perform a GET request\n",
    "        if response.status_code == 200:\n",
    "            return response.content.decode('utf-8')  # Return the content of the response as a string\n",
    "        else:\n",
    "            print(f\"Error: HTTP GET request failed with status code {response.status_code}\")\n",
    "            return None\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error: Failed to fetch URL {full_url}. Exception: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ceed779-8cc4-4b76-8f87-46138a9c24b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content retrieved successfully:\n"
     ]
    }
   ],
   "source": [
    "# Replace this with the specific endpoint or URL you want to fetch\n",
    "url: str = \"https://em360tech.com/tech-article/what-is-llama-3\"  \n",
    "content: Optional[str] = fetch_url_content(url)\n",
    "\n",
    "\n",
    "if content is not None:\n",
    "    print(\"Content retrieved successfully:\")\n",
    "else:\n",
    "    print(\"Failed to retrieve content from the specified URL.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9087aba-d24a-49c8-acee-8ad3e17a6214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: What is Llama 3? Everything you Need to Know About Meta\\'s New AI\\n\\nURL Source: https://em360tech.com/tech-article/what-is-llama-3\\n\\nPublished Time: 2024-11-26T09:45:00+00:00\\n\\nMarkdown Content:\\nWhat is Meta\\'s Llama 3? Everything you Need to Know | Enterprise Tech News EM360Tech\\n===============\\n\\n[Skip to main content](https://em360tech.com/tech-article/what-is-llama-3#main-content)\\n\\n[![Image 1: Home](https://em360tech.com/themes/custom/tech360/images/logos/site-logo-black-100.webp)Home](https://em360tech.com/ \"Home\")\\n\\n*   [Podcasts](https://em360tech.com/podcasts)\\n*   [Tech Articles](https://em360tech.com/tech-articles)\\n*   [Top 10s](https://em360tech.com/top-10)\\n*   [Content](https://em360tech.com/tech-article/what-is-llama-3#)\\n\\nTopics \\n    *   [AI](https://em360tech.com/ai-feed)\\n    *   [Data](https://em360tech.com/data-feed)\\n    *   [Emerging Technologies](https://em360tech.com/emerging-technologies-feed)\\n    *   [Infrastructure Management](https://em360tech.com/infrastructure-management-feed)\\n    *   [Security](https://em360tech.com/security-feed)\\n    *   [In The News](https://em360tech.com/in-the-news-feed)\\n\\nMedia\\n\\n    *   [Tech Articles](https://em360tech.com/tech-articles)\\n    *   [Podcasts](https://em360tech.com/podcasts)\\n    *   [Top 10s](https://em360tech.com/top-10)\\n    *   [Whitepapers](https://em360tech.com/whitepapers)\\n    *   [Press Releases](https://em360tech.com/press-releases)\\n    *   [Video Resources](https://em360tech.com/video-resources)\\n\\n*   [Solution Providers](https://em360tech.com/solution-providers)\\n*   [Industry Leaders](https://em360tech.com/industry-leaders)\\n\\n*   [Sign In](https://em360tech.com/user/login)\\n*   [Join the community](https://em360tech.com/community)\\n\\nClose\\n\\n![Image 2: em360tech.com](https://em360tech.com/themes/custom/tech360/images/logos/site-logo-black.png)\\n\\nGet Started\\n\\n*   Create Account\\n*   Sign In\\n\\n Individual \\n\\n*   Access expert IT resources for free\\n*   Discover actionable insights from industry experts\\n*   Network with IT professionals and grow your network\\n*   Unlock exclusive analyst-led content\\n*   Share your thoughts with our global IT community\\n\\n Business \\n\\nComing Soon...\\n\\n*   Access free expert IT resources\\n*   Network with professionals and industry leaders\\n*   Gain exclusive insights from a global IT community\\n\\nUsername \\n\\nPassword \\n\\nOr\\n\\nSign up with LinkedIn\\n\\nSign up with Github\\n\\nSign up with Google\\n\\n[Signup as business](https://em360tech.com/tech-article/what-is-llama-3#)\\n\\n Already have account? Login\\n\\n[Privacy Policy](https://em360tech.com/privacy-policy)\\n\\n[Contact us](https://em360tech.com/contact)\\n\\nShowing results for\\n\\n*   [Content](https://em360tech.com/quicktabs/nojs/global_search/0)\\n*   [Industry experts](https://em360tech.com/quicktabs/nojs/global_search/1)\\n*   [Solution providers](https://em360tech.com/quicktabs/nojs/global_search/2)\\n*   [Events](https://em360tech.com/quicktabs/nojs/global_search/3)\\n\\nNo matches found\\n\\nExplore by topics\\n\\nAll \\n\\nAI \\n\\nEMERGING TECHNOLOGIES \\n\\nSECURITY \\n\\nDATA \\n\\nINFRASTRUCTURE MANAGEMENT \\n\\nIN THE NEWS \\n\\nExplore by type\\n\\nAll \\n\\nTech Articles \\n\\nPodcast \\n\\nTop 10 \\n\\nWhitepapers \\n\\nVideo Resource \\n\\nNo matches found\\n\\nNo matches found\\n\\nExplore by topics\\n\\nALL \\n\\nAI \\n\\nDATA \\n\\nEMERGING TECHNOLOGIES \\n\\nINFRASTRUCTURE MANAGEMENT \\n\\nSECURITY \\n\\nIN THE NEWS \\n\\nNo matches found\\n\\nExplore by topics\\n\\nALL \\n\\nDATA \\n\\nAI \\n\\nEMERGING TECHNOLOGIES \\n\\nINFRASTRUCTURE MANAGEMENT \\n\\nSECURITY \\n\\nNo result found for\\n\\n[Podcasts](https://em360tech.com/podcasts)\\n\\n[Tech articles](https://em360tech.com/tech-articles)\\n\\n[Top 10s](https://em360tech.com/top-10)\\n\\n Topics \\n\\n*   [AI](https://em360tech.com/ai-feed)\\n*   [Data](https://em360tech.com/data-feed)\\n*   [Emerging Technologies](https://em360tech.com/emerging-technologies-feed)\\n*   [Infrastructure Management](https://em360tech.com/infrastructure-management-feed)\\n*   [Security](https://em360tech.com/security-feed)\\n*   [In the news](https://em360tech.com/in-the-news-feed)\\n\\n Media Types \\n\\n*   [Press release](https://em360tech.com/press-releases)\\n*   [Video resources](https://em360tech.com/video-resources)\\n*   [Whitepapers](https://em360tech.com/whitepapers)\\n\\n[Industry Leaders](https://em360tech.com/industry-leaders)\\n\\n[Media partners](https://em360tech.com/media-partners)\\n\\n[Solution Providers](https://em360tech.com/solution-providers)\\n\\n[Events](https://em360tech.com/events)\\n\\n More \\n\\n*   [About us](https://em360tech.com/about-us)\\n*   [Contact us](https://em360tech.com/contact)\\n*   [FAQs](https://em360tech.com/faqs)\\n\\n[The Compliance Conundrum in the Cloud Era: Governance and Adapting to Regulatory Volatility](https://em360tech.com/whitepapers/compliance-conundrum-report-2025)\\n\\n![Image 3: em360tech image](https://em360tech.com/sites/default/files/styles/content_card_primary/public/2025-10/em360tech_em360tech-compliance-conundrum-cloud-era-report-2025_header-combined.jpg.webp?h=12272a73&itok=mIcWYtm4)\\n\\nWhitepaper\\n\\n Security\\n\\n### The Compliance Conundrum in the Cloud Era: Governance and Adapting to Regulatory Volatility\\n\\nby Megan Leanda Berry\\n\\n 4 min \\n\\nAutomate compliance. Strengthen resilience. \\n\\nGet the new EM360Tech report on cloud and hybrid compliance. Learn how to replace manual audits with continuous assurance, automate evidence collection, and govern controls with confidence in a volatile regulatory landscape.\\n\\n*   Why manual compliance can’t keep up\\n*   How automation transforms assurance and reporting\\n*   Practical governance models for automated controls\\n*   Managing data sovereignty across jurisdictions\\n*   Strategies for adapting to regulatory change\\n\\n[Get The Report](https://em360tech.com/whitepapers/compliance-conundrum-report-2025)\\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\n*   [Tech Article](https://em360tech.com/tech-articles)\\n*   /\\n*   [AI](https://em360tech.com/ai-feed)\\n\\nWhat is Llama 3? Everything you Need to Know About Meta\\'s New AI\\n================================================================\\n\\n[![Image 4: Katie Baker](https://em360tech.com/sites/default/files/styles/author_image/public/linkedinImages/user-picture-4489-1706648130914.jpeg.webp?itok=qhaiTWxJ) Katie Baker](https://em360tech.com/user/katie-baker)\\n\\n 26 November 2024 \\n\\n6 min \\n\\n 4544 \\n\\n4\\n\\nShare on Twitter Share on LinkedIn Copy link\\n\\nLink copied to clipboard!\\n\\nMeta has just announced the launch of Llama 3, the next generation of their large language model that brings a host of new AI features to Meta\\'s social platforms.\\n\\nThe powerful AI, trained on a massive dataset of text and code,not only boasts improved capabilities but is also integrated into Meta\\'s core social media platforms – Facebook, Instagram, and WhatsApp – as their new AI assistant, \"Meta AI.\"\\n\\nThe announcement also came with talks of an even more powerful Llama on the horizon. Meta has hinted at a 400 billion parameter model still in training, suggesting their commitment to pushing the boundaries of AI.\\n\\n![Image 5: em360tech image](https://em360tech.com/sites/default/files/styles/cover_image/public/2024-04/what-is-meta-llama-3.jpg.webp?h=23fef6bf&itok=KhHprPEW)\\n\\nIn this article, we will explore **Meta Llama 3, how to use it** and the differences between Llama 2 and Llama 3.\\n\\nWhat is Meta Llama 3?\\n---------------------\\n\\nMeta Llama 3 is a [large language model (LLM)](https://em360tech.com/node/28349) developed by Meta that\\'s trained on a massive amount of text data.\\n\\nThis allows it to understand and respond to language in a comprehensive way, making it suitable for tasks like writing different kinds of creative content, translating languages, and answering your questions in an informative way.\\n\\nLlama 3 models will be available on AWS, Databricks, Google Cloud, Hugging Face, Kaggle, IBM WatsonX, Microsoft Azure, NVIDIA NIM, and Snowflake.\\n\\nThe development of Llama 3 represents a significant advancement in LLM technology, especially for Meta. Its openness encourages collaboration and paves the way for even more powerful and versatile AI tools in the future. As research and development progress, we can expect even more innovative applications for Llama 3 across various industries.\\n\\nLlama 3 vs Llama 2: Which is better?\\n------------------------------------\\n\\nCompared to previous versions like Llama 2, Llama 3 boasts better reasoning abilities, code generation, and can follow instructions more effectively.It also outperforms other open models on benchmarks that measure language understanding and response (ARC, DROP and MMLU).\\n\\nOne of the most intriguing new feature of Llama 3 compared to Llama 2 is its integration into Meta\\'s core products. The AI assistant is now accessible through chat functions in Facebook Messenger, Instagram, and WhatsApp.\\n\\nThis translates to a more helpful and more accessible AI assistant. Imagine asking Facebook for a summary of a research paper, or requesting creative writing prompts on Instagram – Llama 3 aims to be there for these tasks and more.\\n\\n**[Read: Llama 2: Everything You Need To Know About Meta\\'s ChatGPT Rival](https://em360tech.com/node/28277)**\\n\\nLlama 3 is able to follow instructions and complete multi-step tasks more effectively and can generate various creative text formats like poems, code, scripts, and more. Crucially, researchers can access and build upon Llama 3, fostering further AI development.\\n\\nLlama 3 vs Llama 2: Key Differences\\n-----------------------------------\\n\\n### 1. Strong Benchmarks\\n\\nStrong benchmarks refer to standardized tests that evaluate an LLM\\'s capabilities across various aspects of language processing. These benchmarks help researchers and developers compare different LLMs and measure their progress over time.\\n\\n**MMLU** benchmarks to assess how well an LLM grasps the meaning and context of a given prompt or question. Whereas, **ARC** and**DROP** benchmarks evaluate an LLM\\'s ability to apply logical reasoning and solve problems. ARC stands for **Abstract Reasoning Corpus** and might involve solving puzzles or answering questions that require making inferences based on the information provided.\\n\\nDROP stands for **Dynamic Reasoning Over Paragraph** and could involve tasks like identifying the cause-and-effect relationships within a passage.\\n\\nStrong benchmarks on established tests like MMLU, ARC, and DROP indicate that Meta Llama 3 performs well in areas like language comprehension, reasoning, and potentially factual knowledge retrieval.\\n\\n### 2. Creative Text Generation\\n\\nLlama 3 has been trained on a **massive dataset**of text and code, including creative writing examples. This allows it to understand the patterns and structures of different text formats. When you provide a prompt or starting point, Llama 3 can use its knowledge to generate text that adheres to the format and style you specify.\\n\\n**[Read: Is Gemini Racist? Google’s AI Pulled Amidst Bias Allegations](https://em360tech.com/node/29123)**\\n\\nLlama 3 offers a powerful tool for **sparking creative ideas**and generating drafts in various text formats. However, human input and expertise are still essential for refining or completing creative projects.\\n\\n### 3. Open-Source\\n\\nMeta\\'s decision to make Llama 3 **open-source i**s a significant development in the field of large language models (LLMs). Open-source software refers to code or models that are **freely available** for anyone to access, modify, and distribute. This allows researchers, developers, and even the general public to experiment with Llama 3, build upon it, and **contribute** to its further development.\\n\\nThis level of openness allows for both **scrutiny** and **collaboration**. Researchers can examine how Llama 3 works and identify potential biases or limitations. This fosters a collaborative environment where developers can work together to improve the model. This can accelerate innovation in the field of AI and LLM development.\\n\\n![Image 6: llama 2 vs llama 3](https://em360tech.com/sites/default/files/inline-images/llama-2-vs-llama-3.jpg)\\n\\nResearchers and students can also learn how LLMs work by studying its code and experimenting with it. This level of openness allows for ongoing discussions and evaluations regarding the ethical implications of AI development and the potential biases within the model.\\n\\n### 4. Integration\\n\\nMeta has integrated Llama 3 into their virtual assistant, \"Meta AI,\" making it able to answer questions and complete tasks with greater understanding. This also means it is available across Meta platforms like Facebook Messenger and will potentially be integrated into search features in the future. Ultimately it is likely that Llama will be integrated automatically in ways that benefit the user without them having to understand how they are using the LLM.\\n\\n**[Read:Google is Banning Gemini from Talking About Elections](https://em360tech.com/node/29175)**\\n\\nWhat is Meta Llama 3.2?\\n-----------------------\\n\\nMeta Llama 3.2 is the latest update to the tech giants large language model. It features groundbreaking multimodal capabilities, alongside improved performance and more language support. The model is set to demonstrate a significant step forward in LLMs.\\n\\n Are you enjoying the content so far? \\n\\n4\\n\\n Why not support Katie Baker by giving this content a like \\n\\nLlama 3.2 introduces the LLM’s ‘native understanding’ of visual inputs. This means it an accurately describe an image, provide answers to questions based on a visual input as well as understand embedded images in text based documents.\\n\\nThis visual understanding will also be implemented in Meta’s AI image editing tool. This will allow users to alter an image based on text inputs, the example given at Meta Connect 2024 shows a picture of a man with the text input ‘change the shoes to neon rollerskates’.\\n\\nLlama 3.2 models can also process and generate longer pieces of text because of their expanded context length of 128K tokens.\\n\\nHow to download Llama 3.2?\\n--------------------------\\n\\nThe open source models of Llama 3.2 are available for download both on llama.com and Hugging Face. They are also available through Meta’s partner platform ecosystem including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud and Snowflake.\\n\\n1.   Visit llama.com\\n2.   Click ‘download models’\\n3.   Fill out your details under the ‘Request Access to Llama Models’\\n4.   Select the model you wish to download from the choices of the lightweight 1B & 3B, the multimodal 11B & 90B and the text based models, 405B, 70B & 8B.\\n5.   Review the Community License Agreement\\n6.   Click accept\\n7.   Follow the instructions to download the model\\n\\nHow to use Llama 3\\n------------------\\n\\nLlama 3 will be available on**all major platforms** including cloud providers and model API providers. You can access the Meta Llama models directly from Meta or through Hugging Face or Kaggle.\\n\\n**How to download Llama 3**from Meta**:**\\n\\n1.   Visit[Meta’s request access form](https://llama.meta.com/llama-downloads).\\n2.   Fill in your information including name, email, date of birth and country.\\n3.   Select the models that you want to access.\\n4.   Review and accept the license agreements.\\n5.   For each model, you will receive an email that contains instructions and a pre-signed URL to download.\\n\\n![Image 7: how to use meta llama 3](https://em360tech.com/sites/default/files/inline-images/how-to-use-meta-llama-3.jpg)\\n\\nMeta’s ‘[Llama Recipes](https://github.com/meta-llama/llama-recipes)’ contains their open-source code that can be leveraged broadly for fine-tuning, deployment and model evaluation.\\n\\nThe arrival of Meta Llama 3 marks a significant step forward in large language model technology.\\n\\nIts focus on reasoning, creative text generation, and open-source accessibility positions it as a powerful tool for various applications. It offers a glimpse into the exciting potential of this evolving technology.\\n\\nAs research and development continue, we can expect even more advancements in the field of LLMs and the way they interact with our world.\\n\\n[![Image 8: Katie Baker](https://em360tech.com/sites/default/files/styles/author_image/public/linkedinImages/user-picture-4489-1706648130914.jpeg.webp?itok=qhaiTWxJ)](https://em360tech.com/user/katie-baker)\\n\\n Katie Baker \\n\\n Head of Content & Communications at EM360Tech \\n\\n[](https://em360tech.com/user/login#sign)Follow\\n\\n[](https://em360tech.com/user/login#sign)Message\\n\\n Did you enjoy the content? \\n\\n4\\n\\n Why not support Katie Baker by giving this content a like \\n\\nComments ( 0 )\\n--------------\\n\\n[Sign in to post a comment](https://em360tech.com/user/login#sign)\\n\\nExplore This Article\\n--------------------\\n\\n1.   [What is Meta Llama 3?](https://em360tech.com/tech-article/what-is-llama-3#toc-heading-6)\\n2.   [Llama 3 vs Llama 2: Which is better?](https://em360tech.com/tech-article/what-is-llama-3#toc-heading-5)\\n3.   [Llama 3 vs Llama 2: Key Differences](https://em360tech.com/tech-article/what-is-llama-3#toc-heading-4)\\n4.   [What is Meta Llama 3.2?](https://em360tech.com/tech-article/what-is-llama-3#toc-heading-3)\\n5.   [How to download Llama 3.2?](https://em360tech.com/tech-article/what-is-llama-3#toc-heading-2)\\n6.   [How to use Llama 3](https://em360tech.com/tech-article/what-is-llama-3#toc-heading-1)\\n\\nSubscribe to our Newsletter\\n---------------------------\\n\\nName \\n\\nE-mail \\n\\n[EM360Tech Homepage![Image 9: em360tech.com](https://em360tech.com/themes/custom/tech360/images/logos/site-logo-black.png)](https://em360tech.com/ \"EM360Tech Homepage\")\\n\\nAccess the latest analyst-led podcasts, tech articles, and industry resources as you connect with some of the brightest minds in enterprise tech.\\n\\n*   [x.com](https://x.com/EM360Tech)\\n*   [LinkedIn](https://www.linkedin.com/company/em360/)\\n*   [YouTube](https://www.youtube.com/channel/UCNCS0CL4v38JWbNaqnn0G4w)\\n\\nTopics\\n------\\n\\n*   [AI](https://em360tech.com/ai-feed)\\n*   [Data](https://em360tech.com/data-feed)\\n*   [Emerging Technologies](https://em360tech.com/emerging-technologies-feed)\\n*   [Infrastructure Management](https://em360tech.com/infrastructure-management-feed)\\n*   [Security](https://em360tech.com/security-feed)\\n\\nResources\\n---------\\n\\n*   [Articles](https://em360tech.com/tech-articles)\\n*   [Podcasts](https://em360tech.com/podcasts)\\n*   [Top 10s](https://em360tech.com/top-10)\\n*   [Videos](https://em360tech.com/video-resources)\\n*   [Whitepapers](https://em360tech.com/whitepapers)\\n\\nCollaborators\\n-------------\\n\\n*   [Events](https://em360tech.com/events)\\n*   [Experts](https://em360tech.com/industry-leaders)\\n*   [Media Partners](https://em360tech.com/media-partners)\\n*   [Solution Providers](https://em360tech.com/solution-providers)\\n\\nCompany\\n-------\\n\\n*   [About](https://em360tech.com/about-us)\\n*   [Blogs](https://em360tech.com/blog)\\n*   [FAQ](https://em360tech.com/faqs)\\n*   [Our Services](https://em360tech.com/our-services)\\n\\nSubscribe to our Newsletter\\n---------------------------\\n\\nName \\n\\nE-mail \\n\\n*   [Contact Us](https://em360tech.com/contact)\\n*   [Our Services](https://em360tech.com/our-services)\\n*   [Blogs](https://em360tech.com/blog)\\n*   [Privacy Policy](https://em360tech.com/privacy-policy \"Privacy Policy\")\\n*   [Editorial Policy](https://em360tech.com/editorial-policy)\\n*   [GDPR Policy](https://em360tech.com/gdpr-policy \"GDPR Policy\")\\n*   [Sitemap](https://em360tech.com/sitemap \"Sitemap\")\\n\\nAll rights reserved. ©2026\\n\\nEnterprise Management 360\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde00dd-2d9e-4fa6-b006-0ee0aac53082",
   "metadata": {},
   "source": [
    "## Split the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c005dd7-c84f-4269-a4b5-9db5c5cbad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae087266-156b-4a10-ba22-26bd0cb2e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_size = 150\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            model_name=\"gpt-4\",\n",
    "            chunk_size=token_size,\n",
    "            chunk_overlap=0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b5d1d70-51cb-421c-9c60-d6f663831665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove all newline characters\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading and trailing spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2878fe14-0e33-4cd8-966a-57fc2d89c82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 40\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_splitter.split_text(content)\n",
    "print(f\"Total chunks: {len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d95465d-0d3f-4912-a460-793db2c7f4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Title: What is Llama 3? Everything you Need to Know About Meta's New AI\\n\\nURL Source: https://em360tech.com/tech-article/what-is-llama-3\\n\\nPublished Time: 2024-11-26T09:45:00+00:00\\n\\nMarkdown Content:\\nWhat is Meta's Llama 3? Everything you Need to Know | Enterprise Tech News EM360Tech\\n===============\\n\\n[Skip to main content](https://em360tech.com/tech-article/what-is-llama-3#main-content)\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34c0593c-1853-40b9-8f4d-b018faaaa8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model=\"text-embedding-3-small\", api_key=\"your-api-key\"):\n",
    "    # Define the API URL\n",
    "    url = \"https://api.openai.com/v1/embeddings\"\n",
    "    \n",
    "    # Prepare headers with the API key\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    \n",
    "    # Prepare the request body\n",
    "    data = {\n",
    "        \"input\": texts,\n",
    "        \"model\": model\n",
    "    }\n",
    "    \n",
    "    # Send a POST request to the OpenAI API\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Return the embeddings from the response\n",
    "        return response.json()[\"data\"]\n",
    "    else:\n",
    "        # Print error if the request fails\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2b2ff15-b577-43ba-ad6c-f2c3165249f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "\n",
    "def ingest_data_into_es(texts):\n",
    "    if not es.indices.exists(index=elastic_index_name):\n",
    "        print(\"The index does not exist, going to generate embeddings\")   \n",
    "        docsearch = ElasticsearchStore.from_texts( \n",
    "            texts,\n",
    "            embedding = embeddings, \n",
    "            es_url = url, \n",
    "            es_connection = es,\n",
    "            index_name = elastic_index_name, \n",
    "            es_user = ES_USER,\n",
    "            es_password = ES_PASSWORD\n",
    "    )\n",
    "    else: \n",
    "        print(\"The index already existed\")\n",
    "    \n",
    "        docsearch = ElasticsearchStore(\n",
    "            es_connection=es,\n",
    "            embedding=embeddings,\n",
    "            es_url = url, \n",
    "            index_name = elastic_index_name, \n",
    "            es_user = ES_USER,\n",
    "            es_password = ES_PASSWORD    \n",
    "        )\n",
    "\n",
    "    return docsearch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75bca4ca-dd8f-4d50-bb5b-a8709db992eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index already existed\n"
     ]
    }
   ],
   "source": [
    "docsearch = ingest_data_into_es(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892195f1-e07d-4adf-8fc7-4de9dd03a7ab",
   "metadata": {},
   "source": [
    "# Search for questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cacfe61-d978-4a94-a570-179ccc26d94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(str):\n",
    "    docs = docsearch.similarity_search(str)\n",
    "    # similarity_threshold = 0.88 \n",
    "    # docs = docsearch.similarity_search_with_relevance_scores(str, score_threshold=similarity_threshold)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca41a21e-0989-4308-a9a4-7b135d6e04a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a6db49-a39f-4c9a-ba77-0621173d6068",
   "metadata": {},
   "source": [
    "# Prompts\n",
    "\n",
    "1. First prompt will check to see if the *retrieved context* can answer the user question.\n",
    "2. Second prompt will get the context and question and generates the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a661fb9b-c61c-4f8f-bed3-71d3622464aa",
   "metadata": {},
   "source": [
    "## First Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b6cb2c1-8778-477b-9427-096ef14a2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_system_prompt = \"\"\"Your job is decide if a given question can be answered with a given context. \n",
    "If context can answer the question return 1.\n",
    "If not return 0.\n",
    "\n",
    "Do not return anything else except for 0 or 1.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87da197-4516-47a5-8c78-2f5101a4488b",
   "metadata": {},
   "source": [
    "## Second Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46f9c6e4-857b-4d1c-a12e-e5406d4326a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an expert for answering questions. Answer the question according only to the given context.\n",
    "If question cannot be answered using the context, simply say I don't know. Do not make stuff up.\n",
    "Your answer MUST be informative, concise, and action driven. Your response must be in Markdown.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6446152-ce5d-4b31-843d-3f8729d4f24c",
   "metadata": {},
   "source": [
    "## Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a20fbcb-2c1e-4fcc-ae94-c391b6981546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def azure_openai_completion(question, context, is_system_prompt=False):\n",
    "    prompt = system_prompt if is_system_prompt else decision_system_prompt\n",
    "    summary = chat.chat.completions.create(\n",
    "    model = AZURE_DEPLOYMENT_ID,\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt.format(context=context) },\n",
    "            {\"role\": \"user\", \"content\": user_prompt.format(question=question)},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # print(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "792c1523-b8b0-4723-99ce-7a2d01d7e9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is openai o1 model\"\n",
    "results = search(question)\n",
    "context = format_docs(results)\n",
    "response = azure_openai_completion(question, context)\n",
    "\n",
    "has_answer = response.choices[0].message.content\n",
    "has_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f71d9a4-8b74-4741-b0d4-5f92bb8cea93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question = \"what is Llama 3?\"\n",
    "# question = \"what is openai o1 model\"\n",
    "# question = \"中国最长的河流是那条河？\"\n",
    "question = \"What is the latest version of Elastic Stack？\"\n",
    "results = search(question)\n",
    "context = format_docs(results)\n",
    "response = azure_openai_completion(question, context)\n",
    "\n",
    "has_answer = response.choices[0].message.content\n",
    "has_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9213c5a5-c8f7-49cb-975b-dbdbc8320755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "import requests\n",
    "\n",
    "def tavily_search(query, max_results=5):\n",
    "    \"\"\"\n",
    "    Call Tavily Search API and return search results.\n",
    "\n",
    "    :param query: search query string\n",
    "    :param api_key: Tavily API key\n",
    "    :param max_results: number of results to return\n",
    "    :return: response JSON (dict)\n",
    "    \"\"\"\n",
    "    url = \"https://api.tavily.com/search\"\n",
    "\n",
    "    payload = {\n",
    "        \"api_key\": TAVILIO_API_KEY,\n",
    "        \"query\": query,\n",
    "        \"max_results\": max_results\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26d7b4-9f92-45d5-8f75-c5a8e8026824",
   "metadata": {},
   "source": [
    "# Check to see if retrieved context can answer the question or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58a0852e-2e07-4636-8f8d-050a269780d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the latest version of Elastic Stack？\n",
      "Context is NOT relevant. Searching online...\n",
      "{'query': 'What is the latest version of Elastic Stack？', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.elastic.co/blog/elastic-stack-9-2-5-released', 'title': 'Elastic Stack 9.2.5 released', 'content': 'Version 9.2.5 of the Elastic Stack was released today. We recommend you upgrade to this latest version. We recommend 9.2.5 over the previous', 'score': 0.88420963, 'raw_content': None}, {'url': 'https://en.wikipedia.org/wiki/Elasticsearch', 'title': 'Elasticsearch', 'content': 'In January 2021, Elastic announced that starting with version 7.11, they ... \"Elastic brings order to its product line with Elastic Stack\". TechCrunch', 'score': 0.71995544, 'raw_content': None}, {'url': 'https://logz.io/learn/complete-guide-elk-stack/', 'title': 'The Complete Guide to the ELK Stack', 'content': 'In early 2021, Elastic announced a bombshell in the open source world: the ELK Stack would no longer be open source, as of version 7.11.', 'score': 0.69573146, 'raw_content': None}, {'url': 'https://www.elastic.co/downloads/elasticsearch', 'title': 'Download Elasticsearch', 'content': '... ELK stack) for free and start searching and analyzing in minutes with Elastic ... Summary. Version: 9.3.0. View past releases. Release date: February 03, 2026.', 'score': 0.6821721, 'raw_content': None}, {'url': 'https://endoflife.date/elasticsearch', 'title': 'Elasticsearch', 'content': 'Elastic Stack product releases follow Semantic Versioning. Elastic ... For example, if version 1.0 was released on 10-Apr-2019 and version 2.0 was', 'score': 0.63397753, 'raw_content': None}], 'response_time': 0.8, 'request_id': '09ff7f6f-148b-4d11-8f38-d45ac4020ee0'}\n",
      "Found online sources. Generating the response...\n",
      "Answer:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The latest version of Elastic Stack mentioned in the context is version 9.3.0, with a release date of February 03, 2026."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_search_results(tavily_response):\n",
    "    \"\"\"\n",
    "    Extract and format content from Tavily search response\n",
    "    \"\"\"\n",
    "    docs = tavily_response.get(\"results\", [])\n",
    "    return \"\\n\\n\".join(doc.get(\"content\", \"\") for doc in docs)\n",
    "    \n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "if has_answer == '1':\n",
    "    print(\"Context can answer the question\")\n",
    "    response = azure_openai_completion(question, context, True)\n",
    "    print(\"Answer:\")\n",
    "    display(Markdown(response.choices[0].message.content))\n",
    "else:\n",
    "    print(\"Context is NOT relevant. Searching online...\")\n",
    "    results = tavily_search(question)\n",
    "    print(results)\n",
    "    context = format_search_results(results)\n",
    "    print(\"Found online sources. Generating the response...\")\n",
    "    response = azure_openai_completion(question, context, True)\n",
    "    print(\"Answer:\")\n",
    "    display(Markdown(response.choices[0].message.content))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d1a751-01d3-436c-865b-6174fe0a3afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U duckduckgo_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb19a39a-bfd0-4dd0-ae35-d9d774fbe1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': '中华人民共和国 - 知乎', 'href': 'https://www.zhihu.com/topic/19586942', 'body': '文化大革命之后开始改革开放，逐步确立了中国特色社会主义制度。 中华人民共和国陆地面积约960万平方公里，大陆海岸线1.8万多千米，岛屿岸线1.4万多千米，内海和边海的水域面积约470多万平方千 …'}, {'title': '2025 胡润中国 500 强发布，台积电、腾讯、字节位列前三 ...', 'href': 'https://www.zhihu.com/question/2002714528765457419', 'body': '2026年2月5日 · “如果你想了解中国民营经济的发展，胡润中国500强是一个很好的切入点。 这些企业是中国经济的支柱。 ”胡润集团董事长兼首席调研官胡润介绍，这500家企业吸纳了约1300万名员工， …'}, {'title': '中国的三个缩写 PRC CHN CN，各用在什么场合或领域？', 'href': 'https://www.zhihu.com/question/22379997/answers/updated', 'body': \"2013年12月27日 · PRC, ZRG是国家字母 缩写： PRC是中国 英文 全称the People's Republic of China的缩写，主要用于外交等场合，强调一个中国原则； ZRG是 汉语拼音 全称Zhonghua Renmin Gongheguo …\"}, {'title': '如何看待权威第三方公布的2026年1月华为中国手机市场份额 ...', 'href': 'https://www.zhihu.com/question/2001595690313348791', 'body': '如何看待权威第三方公布的2026年1月华为中国手机市场份额第一？ 2月1日，根据权威第三方销量数据（Sell out）, 华为领跑新年手机市场，市场份额提升至18.6% ，华为再夺1月中国手机市场份额第一。 [ …'}, {'title': '荷兰安世停止向中国工厂供应晶圆，安世中国称「已建立充足 ...', 'href': 'https://www.zhihu.com/question/1968280135229728427', 'body': '2025年11月2日 · 但安世荷兰要完全生产产品，70%左右的封测产能在中国，而且稀土等原材料供应也依赖中国，现在这种情况肯定没法给他们供货，过段时间安世荷兰很可能无法出货。 另外，安世有很多 …'}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (elastic-venv)",
   "language": "python",
   "name": "elastic-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
